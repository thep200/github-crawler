// filepath: /Users/thep200/Projects/Study/github-crawler/internal/crawler/crawler_v3.go
// Crawler version 3
// Crawler v∆∞·ª£t qua gi·ªõi h·∫°n GitHub API b·∫±ng c√°ch s·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c time-based query
// v·ªõi hai giai ƒëo·∫°n song song: ∆∞u ti√™n thu th·∫≠p 5000 repo tr∆∞·ªõc, sau ƒë√≥ x·ª≠ l√Ω commits v√† releases

package crawler

import (
	"context"
	"fmt"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/thep200/github-crawler/cfg"
	githubapi "github.com/thep200/github-crawler/internal/github_api"
	"github.com/thep200/github-crawler/internal/limiter"
	"github.com/thep200/github-crawler/internal/model"
	"github.com/thep200/github-crawler/pkg/db"
	"github.com/thep200/github-crawler/pkg/log"
	"gorm.io/gorm"
)

type CrawlerV3 struct {
	Logger                log.Logger
	Config                *cfg.Config
	Mysql                 *db.Mysql
	RepoMd                *model.Repo
	ReleaseMd             *model.Release
	CommitMd              *model.Commit
	rateLimiter           *limiter.RateLimiter
	processedRepoIDs      map[int64]bool
	processedReleaseKeys  map[string]bool
	processedCommitHashes map[string]bool
	processedLock         sync.RWMutex

	// Worker channels
	repoWorkers    chan struct{}
	releaseWorkers chan struct{}
	commitWorkers  chan struct{}
	errorChan      chan error
	backgroundWg   sync.WaitGroup

	// Counters for tracking progress
	repoCount     int32
	releaseCount  int32
	commitCount   int32
	maxRepos      int32
	pageWorkers   chan struct{}
	pageWaitGroup sync.WaitGroup

	// Time-based crawling
	timeWindows       []timeWindow
	currentWindowLock sync.Mutex
	currentWindowIdx  int

	// Th√™m m·ªôt mutex v√† slice ƒë·ªÉ theo d√µi t·∫•t c·∫£ repositories ƒë√£ crawl
	allReposMutex sync.Mutex
	allRepos      []RepositorySummary

	// Phase control
	repoCollectionDone chan struct{} // Signal khi ƒë√£ thu th·∫≠p ƒë·ªß repos
	secondPhaseDone    chan struct{} // Signal khi phase 2 ho√†n th√†nh
}

// C·∫•u tr√∫c ƒë·ªÉ ƒë·ªãnh nghƒ©a m·ªôt c·ª≠a s·ªï th·ªùi gian cho vi·ªác t√¨m ki·∫øm
type timeWindow struct {
	startDate time.Time
	endDate   time.Time
	processed bool
}

// C·∫•u tr√∫c l∆∞u th√¥ng tin t√≥m t·∫Øt v·ªÅ repository ƒë·ªÉ s·∫Øp x·∫øp theo s·ªë sao
type RepositorySummary struct {
	ID        int64
	Stars     int64
	UserLogin string
	RepoName  string
	APIRepo   githubapi.GithubAPIResponse
}

func NewCrawlerV3(logger log.Logger, config *cfg.Config, mysql *db.Mysql) (*CrawlerV3, error) {
	repoMd, _ := model.NewRepo(config, logger, mysql)
	releaseMd, _ := model.NewRelease(config, logger, mysql)
	commitMd, _ := model.NewCommit(config, logger, mysql)
	rateLimiter := limiter.NewRateLimiter(config.GithubApi.RequestsPerSecond)

	// C·∫•u h√¨nh s·ªë l∆∞·ª£ng worker t·ªëi ƒëa
	maxRepoWorkers := 10
	maxReleaseWorkers := 20
	maxCommitWorkers := 30
	maxPageWorkers := 15

	// T·∫°o c√°c time windows cho vi·ªác t√¨m ki·∫øm
	// C·∫ßn t·∫°o ƒë·ªß windows ƒë·ªÉ bao qu√°t to√†n b·ªô d·ªØ li·ªáu c·∫ßn crawl
	timeWindows := generateTimeWindows()

	return &CrawlerV3{
		Logger:                logger,
		Config:                config,
		Mysql:                 mysql,
		RepoMd:                repoMd,
		ReleaseMd:             releaseMd,
		CommitMd:              commitMd,
		rateLimiter:           rateLimiter,
		processedRepoIDs:      make(map[int64]bool, 10000),
		processedReleaseKeys:  make(map[string]bool, 20000),
		processedCommitHashes: make(map[string]bool, 40000),
		processedLock:         sync.RWMutex{},
		repoWorkers:           make(chan struct{}, maxRepoWorkers),
		releaseWorkers:        make(chan struct{}, maxReleaseWorkers),
		commitWorkers:         make(chan struct{}, maxCommitWorkers),
		pageWorkers:           make(chan struct{}, maxPageWorkers),
		errorChan:             make(chan error, 200),
		backgroundWg:          sync.WaitGroup{},
		repoCount:             0,
		releaseCount:          0,
		commitCount:           0,
		maxRepos:              5000, // M·ª•c ti√™u l√† 5000 repos
		timeWindows:           timeWindows,
		currentWindowIdx:      0,
		allReposMutex:         sync.Mutex{},
		allRepos:              make([]RepositorySummary, 0, 10000), // D·ª± ki·∫øn l∆∞u tr·ªØ nhi·ªÅu h∆°n ƒë·ªÉ c√≥ th·ªÉ s·∫Øp x·∫øp
		repoCollectionDone:    make(chan struct{}),
		secondPhaseDone:       make(chan struct{}),
	}, nil
}

// T·∫°o c√°c kho·∫£ng th·ªùi gian ƒë·ªÉ query GitHub API v·ªõi ∆∞u ti√™n cho repo m·ªõi v√† nhi·ªÅu sao
func generateTimeWindows() []timeWindow {
	windows := []timeWindow{
		// Kho·∫£ng th·ªùi gian ƒë·∫ßu ti√™n: repos r·∫•t m·ªõi v√† r·∫•t ph·ªï bi·∫øn
		{
			startDate: time.Date(2024, 1, 1, 0, 0, 0, 0, time.UTC),
			endDate:   time.Now(),
			processed: false,
		},
		// Kho·∫£ng th·ªùi gian th·ª© hai: 2023 - repos ph·ªï bi·∫øn g·∫ßn ƒë√¢y
		{
			startDate: time.Date(2023, 1, 1, 0, 0, 0, 0, time.UTC),
			endDate:   time.Date(2024, 1, 1, 0, 0, 0, 0, time.UTC),
			processed: false,
		},
		// Kho·∫£ng th·ªùi gian th·ª© ba: 2022
		{
			startDate: time.Date(2022, 1, 1, 0, 0, 0, 0, time.UTC),
			endDate:   time.Date(2023, 1, 1, 0, 0, 0, 0, time.UTC),
			processed: false,
		},
		// Kho·∫£ng th·ªùi gian th·ª© t∆∞: 2021
		{
			startDate: time.Date(2021, 1, 1, 0, 0, 0, 0, time.UTC),
			endDate:   time.Date(2022, 1, 1, 0, 0, 0, 0, time.UTC),
			processed: false,
		},
		// Kho·∫£ng th·ªùi gian th·ª© nƒÉm: 2019-2020
		{
			startDate: time.Date(2019, 1, 1, 0, 0, 0, 0, time.UTC),
			endDate:   time.Date(2021, 1, 1, 0, 0, 0, 0, time.UTC),
			processed: false,
		},
		// Kho·∫£ng th·ªùi gian th·ª© s√°u: 2016-2018
		{
			startDate: time.Date(2016, 1, 1, 0, 0, 0, 0, time.UTC),
			endDate:   time.Date(2019, 1, 1, 0, 0, 0, 0, time.UTC),
			processed: false,
		},
		// Kho·∫£ng th·ªùi gian th·ª© b·∫£y: 2012-2015
		{
			startDate: time.Date(2012, 1, 1, 0, 0, 0, 0, time.UTC),
			endDate:   time.Date(2016, 1, 1, 0, 0, 0, 0, time.UTC),
			processed: false,
		},
		// Kho·∫£ng th·ªùi gian th·ª© t√°m: C≈© nh·∫•t (tr∆∞·ªõc 2012)
		{
			startDate: time.Date(2007, 1, 1, 0, 0, 0, 0, time.UTC),
			endDate:   time.Date(2012, 1, 1, 0, 0, 0, 0, time.UTC),
			processed: false,
		},
	}
	return windows
}

// L·∫•y URL truy v·∫•n v·ªõi th√¥ng s·ªë th·ªùi gian c·ª• th·ªÉ
func (c *CrawlerV3) getTimeBasedQueryURL(window timeWindow) string {
	baseUrl := "https://api.github.com/search/repositories"
	startDate := window.startDate.Format("2006-01-02")
	endDate := window.endDate.Format("2006-01-02")

	// T·∫°o query v·ªõi ƒëi·ªÅu ki·ªán th·ªùi gian
	// q=stars:>1+created:{start_date}..{end_date}&sort=stars&order=desc
	query := fmt.Sprintf("?q=stars:>100+created:%s..%s&sort=stars&order=desc", startDate, endDate)

	return baseUrl + query
}

// L·∫•y time window ti·∫øp theo ƒë·ªÉ x·ª≠ l√Ω
func (c *CrawlerV3) getNextTimeWindow() *timeWindow {
	c.currentWindowLock.Lock()
	defer c.currentWindowLock.Unlock()

	if c.currentWindowIdx >= len(c.timeWindows) {
		return nil
	}

	window := &c.timeWindows[c.currentWindowIdx]
	c.currentWindowIdx++
	return window
}

func (c *CrawlerV3) Crawl() bool {
	ctx := context.Background()
	startTime := time.Now()
	c.Logger.Info(ctx, "B·∫Øt ƒë·∫ßu crawl d·ªØ li·ªáu repository GitHub v·ªõi chi·∫øn l∆∞·ª£c hai giai ƒëo·∫°n %s", startTime.Format(time.RFC3339))

	// T·∫°o context v·ªõi kh·∫£ nƒÉng h·ªßy
	crawlCtx, cancel := context.WithCancel(ctx)
	defer cancel()

	// Kh·ªüi ch·∫°y goroutine gi√°m s√°t l·ªói
	go c.errorMonitor(crawlCtx)

	// K·∫øt n·ªëi database
	db, err := c.Mysql.Db()
	if err != nil {
		c.Logger.Error(ctx, "Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn database: %v", err)
		return false
	}

	// Giai ƒëo·∫°n 1: Thu th·∫≠p th√¥ng tin v·ªÅ repositories
	c.Logger.Info(ctx, "===== GIAI ƒêO·∫†N 1: THU TH·∫¨P TH√îNG TIN REPOSITORIES =====")
	c.collectRepositoriesPhase(ctx, db)

	// Giai ƒëo·∫°n 2: X·ª≠ l√Ω v√† l∆∞u repositories, sau ƒë√≥ thu th·∫≠p releases v√† commits
	c.Logger.Info(ctx, "===== GIAI ƒêO·∫†N 2: X·ª¨ L√ù REPOSITORIES/RELEASES/COMMITS =====")
	c.processRepositoriesPhase(ctx, db)

	// Ghi log k·∫øt qu·∫£
	close(c.errorChan)
	c.logCrawlResults(ctx, startTime)
	return true
}

func (c *CrawlerV3) startTimeWindowWorkers(ctx context.Context, db *gorm.DB, doneCh chan bool) {
	// S·ªë l∆∞·ª£ng time windows x·ª≠ l√Ω ƒë·ªìng th·ªùi
	maxConcurrentWindows := 2

	// S·ªë l∆∞·ª£ng trang t·ªëi ƒëa cho m·ªói time window
	maxPagesPerWindow := 10

	// S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr√™n m·ªói trang
	perPage := 100

	for i := 0; i < maxConcurrentWindows; i++ {
		c.pageWaitGroup.Add(1)
		go func(workerID int) {
			defer c.pageWaitGroup.Done()

			for {
				// L·∫•y time window ti·∫øp theo
				window := c.getNextTimeWindow()
				if window == nil {
					return
				}

				c.Logger.Info(ctx, "Worker %d b·∫Øt ƒë·∫ßu x·ª≠ l√Ω time window t·ª´ %s ƒë·∫øn %s",
					workerID, window.startDate.Format("2006-01-02"), window.endDate.Format("2006-01-02"))

				// Th·ª±c hi·ªán crawl cho time window n√†y
				url := c.getTimeBasedQueryURL(*window)

				// Override URL trong config
				configCopy := *c.Config
				configCopy.GithubApi.ApiUrl = url

				var startPage int32 = 0
				var windowPageWg sync.WaitGroup

				for p := 0; p < maxPagesPerWindow; p++ {
					windowPageWg.Add(1)
					go func() {
						defer windowPageWg.Done()

						for {
							currentPage := atomic.AddInt32(&startPage, 1)
							if currentPage > 10 { // GitHub ch·ªâ cho ph√©p t·ªëi ƒëa 10 trang
								return
							}

							select {
							case <-doneCh:
								return
							case <-ctx.Done():
								return
							case c.pageWorkers <- struct{}{}:
								c.crawlTimeWindowPage(ctx, db, int(currentPage), perPage, &configCopy, doneCh)
								<-c.pageWorkers
							}
						}
					}()
				}

				windowPageWg.Wait()
			}
		}(i)
	}
}

func (c *CrawlerV3) crawlTimeWindowPage(ctx context.Context, db *gorm.DB, page, perPage int, config *cfg.Config, doneCh chan bool) {
	// Ki·ªÉm tra xem ƒë√£ thu th·∫≠p ƒë·ªß d·ªØ li·ªáu ch∆∞a
	select {
	case <-doneCh:
		return // ƒê√£ thu th·∫≠p ƒë·ªß d·ªØ li·ªáu, d·ª´ng x·ª≠ l√Ω
	default:
		// Ti·∫øp t·ª•c x·ª≠ l√Ω
	}

	// √Åp d·ª•ng rate limiting
	c.applyRateLimit()

	// T·∫°o caller m·ªõi v·ªõi URL time-based
	apiCaller := githubapi.NewCaller(c.Logger, config, page, perPage)

	// G·ªçi API
	repos, err := apiCaller.Call()
	if err != nil {
		if c.isRateLimitError(err) {
			c.Logger.Warn(ctx, "Rate limit hit, sleeping for 60 seconds: %v", err)
			time.Sleep(60 * time.Second)
			repos, err = apiCaller.Call()
			if err != nil {
				c.Logger.Error(ctx, "Error after rate limit wait: %v", err)
				return
			}
		} else {
			c.Logger.Error(ctx, "Error calling GitHub API: %v", err)
			return
		}
	}

	// Kh√¥ng c√≥ k·∫øt qu·∫£
	if len(repos) == 0 {
		c.Logger.Info(ctx, "No repositories found for page %d", page)
		return
	}

	c.Logger.Info(ctx, "Found %d repositories on page %d", len(repos), page)

	// Thu th·∫≠p th√¥ng tin repositories ƒë·ªÉ sau n√†y s·∫Øp x·∫øp v√† x·ª≠ l√Ω
	c.allReposMutex.Lock()
	initialCount := len(c.allRepos)

	for _, repo := range repos {
		// Ch·ªâ thu th·∫≠p th√¥ng tin, ch∆∞a x·ª≠ l√Ω repository
		if repo.StargazersCount < 100 {
			continue // B·ªè qua repositories c√≥ √≠t h∆°n 100 sao ƒë·ªÉ t·∫≠p trung v√†o nh·ªØng repo n·ªïi b·∫≠t
		}

		c.allRepos = append(c.allRepos, RepositorySummary{
			ID:        repo.Id,
			Stars:     repo.StargazersCount,
			UserLogin: repo.Owner.Login,
			RepoName:  repo.Name,
			APIRepo:   repo,
		})
	}

	totalCollected := len(c.allRepos)
	newRepos := totalCollected - initialCount
	c.allReposMutex.Unlock()

	c.Logger.Info(ctx, "ƒê√£ thu th·∫≠p th√™m %d repositories (t·ªïng c·ªông: %d)", newRepos, totalCollected)

	// Ki·ªÉm tra xem ƒë√£ thu th·∫≠p ƒë·ªß d·ªØ li·ªáu ch∆∞a
	if totalCollected >= 10000 { // Thu th·∫≠p ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ l·ªçc 5000 repos t·ªët nh·∫•t
		select {
		case <-doneCh: // ƒê√£ ƒë√≥ng b·ªüi goroutine kh√°c
		default:
			close(doneCh) // ƒê√≥ng channel ƒë·ªÉ b√°o hi·ªáu ƒë√£ thu th·∫≠p ƒë·ªß d·ªØ li·ªáu
		}
	}
}

func (c *CrawlerV3) crawlReleasesAndCommitsAsync(ctx context.Context, db *gorm.DB, user, repoName string, repoID int) {
	// Gi·ªõi h·∫°n th·ªùi gian t·ªëi ƒëa cho m·ªói repository
	timeoutCtx, cancel := context.WithTimeout(ctx, 30*time.Minute)
	defer cancel()

	// T·∫°o caller m·ªõi cho releases
	apiCaller := githubapi.NewCaller(c.Logger, c.Config, 1, 100)

	// X·ª≠ l√Ω releases v√† commits
	var wg sync.WaitGroup
	wg.Add(1)

	// Goroutine x·ª≠ l√Ω releases
	go func() {
		defer wg.Done()
		releases, err := c.crawlReleases(timeoutCtx, db, apiCaller, user, repoName, repoID)
		if err != nil {
			c.Logger.Warn(timeoutCtx, "L·ªói khi crawl releases cho %s/%s: %v", user, repoName, err)
		} else {
			c.Logger.Info(timeoutCtx, "ƒê√£ crawl %d releases cho %s/%s", len(releases), user, repoName)
		}
	}()

	// ƒê·ª£i t·∫•t c·∫£ goroutines ho√†n th√†nh
	wg.Wait()
}

func (c *CrawlerV3) errorMonitor(ctx context.Context) {
	for {
		select {
		case err, ok := <-c.errorChan:
			if !ok {
				return
			}
			if err != nil {
				c.Logger.Error(ctx, "L·ªói trong worker: %v", err)
			}
		case <-ctx.Done():
			return
		}
	}
}

func (c *CrawlerV3) isProcessed(repoID int64) bool {
	c.processedLock.RLock()
	defer c.processedLock.RUnlock()
	return c.processedRepoIDs[repoID]
}

func (c *CrawlerV3) addProcessedID(repoID int64) {
	c.processedLock.Lock()
	defer c.processedLock.Unlock()
	c.processedRepoIDs[repoID] = true
}

// Check if a release has been processed
func (c *CrawlerV3) isReleaseProcessed(repoID int, releaseName string) bool {
	key := fmt.Sprintf("%d_%s", repoID, releaseName)
	c.processedLock.RLock()
	defer c.processedLock.RUnlock()
	return c.processedReleaseKeys[key]
}

func (c *CrawlerV3) addProcessedRelease(repoID int, releaseName string) {
	key := fmt.Sprintf("%d_%s", repoID, releaseName)
	c.processedLock.Lock()
	defer c.processedLock.Unlock()
	c.processedReleaseKeys[key] = true
}

func (c *CrawlerV3) isCommitProcessed(commitHash string) bool {
	c.processedLock.RLock()
	defer c.processedLock.RUnlock()
	return c.processedCommitHashes[commitHash]
}

func (c *CrawlerV3) addProcessedCommit(commitHash string) {
	c.processedLock.Lock()
	defer c.processedLock.Unlock()
	c.processedCommitHashes[commitHash] = true
}

func (c *CrawlerV3) applyRateLimit() {
	attempts := 0
	maxAttempts := 5
	baseDelay := time.Duration(c.Config.GithubApi.ThrottleDelay) * time.Millisecond
	for !c.rateLimiter.Allow() {
		attempts++
		if attempts > maxAttempts {
			time.Sleep(5 * time.Second)
			attempts = 0
		} else {
			delay := baseDelay * time.Duration(attempts)
			time.Sleep(delay)
		}
	}
}

func (c *CrawlerV3) isRateLimitError(err error) bool {
	return strings.Contains(err.Error(), "403") ||
		strings.Contains(err.Error(), "rate limit") ||
		strings.Contains(err.Error(), "ƒë·∫°t gi·ªõi h·∫°n API")
}

func (c *CrawlerV3) handleRateLimit(ctx context.Context, err error) {
	if c.isRateLimitError(err) {
		waitMinutes := c.Config.GithubApi.RateLimitResetMin
		if waitMinutes <= 0 {
			waitMinutes = 60 // M·∫∑c ƒë·ªãnh 60 ph√∫t n·∫øu kh√¥ng c√≥ c·∫•u h√¨nh
		}

		// L·∫•y th·ªùi gian reset c·ª• th·ªÉ n·∫øu c√≥
		var resetTime time.Time
		var resetTimeStr string
		if strings.Contains(err.Error(), "th·ªùi gian reset:") {
			parts := strings.Split(err.Error(), "th·ªùi gian reset:")
			if len(parts) > 1 {
				resetTimeStr = strings.TrimSpace(parts[1])
				parsedTime, parseErr := time.Parse(time.RFC3339, resetTimeStr)
				if parseErr == nil {
					resetTime = parsedTime
				}
			}
		}

		// T√≠nh to√°n th·ªùi gian ch·ªù
		waitTime := time.Duration(waitMinutes) * time.Minute
		if !resetTime.IsZero() {
			// N·∫øu c√≥ th·ªùi gian reset c·ª• th·ªÉ, s·ª≠ d·ª•ng n√≥
			now := time.Now()
			calculatedWaitTime := resetTime.Sub(now)
			if calculatedWaitTime > 0 {
				waitTime = calculatedWaitTime
			}
		}

		c.Logger.Warn(ctx, "üö´ Rate limit c·ªßa GitHub API ƒë·∫°t ng∆∞·ª°ng. Ch·ªù %v ƒë·ªÉ ti·∫øp t·ª•c (ƒë·∫øn %s)",
			waitTime.Round(time.Second), time.Now().Add(waitTime).Format(time.RFC3339))

		time.Sleep(waitTime)

		c.Logger.Info(ctx, "‚úÖ ƒê√£ h·∫øt th·ªùi gian ch·ªù rate limit, ti·∫øp t·ª•c crawl")
	}
}

func (c *CrawlerV3) processTopRepositories(ctx context.Context, db *gorm.DB) {
	c.Logger.Info(ctx, "B·∫Øt ƒë·∫ßu x·ª≠ l√Ω top repositories theo s·ªë sao trong giai ƒëo·∫°n 2")

	c.allReposMutex.Lock()
	// S·∫Øp x·∫øp repositories theo s·ªë sao gi·∫£m d·∫ßn
	sort.Slice(c.allRepos, func(i, j int) bool {
		return c.allRepos[i].Stars > c.allRepos[j].Stars
	})

	// Gi·ªõi h·∫°n ch·ªâ l·∫•y 5000 repositories c√≥ s·ªë sao cao nh·∫•t
	topRepos := c.allRepos
	if len(topRepos) > int(c.maxRepos) {
		topRepos = topRepos[:c.maxRepos]
		c.Logger.Info(ctx, "ƒê√£ l·ªçc xu·ªëng c√≤n %d repositories c√≥ s·ªë sao cao nh·∫•t t·ª´ %d repositories ƒë√£ thu th·∫≠p",
			c.maxRepos, len(c.allRepos))
	} else {
		c.Logger.Info(ctx, "C√≥ t·ªïng c·ªông %d repositories, t·∫•t c·∫£ ƒë·ªÅu ƒë∆∞·ª£c x·ª≠ l√Ω", len(topRepos))
	}
	c.allReposMutex.Unlock()

	// Reset counter ƒë·ªÉ ƒë·∫øm l·∫°i ch√≠nh x√°c
	atomic.StoreInt32(&c.repoCount, 0)

	// Semaphore cho vi·ªác x·ª≠ l√Ω repo
	repoSemaphore := make(chan struct{}, cap(c.repoWorkers))

	// T·∫°o m·ªôt worker pool ƒë·ªÉ x·ª≠ l√Ω repo
	c.Logger.Info(ctx, "Kh·ªüi t·∫°o %d worker ƒë·ªÉ l∆∞u repositories v√†o database", cap(c.repoWorkers))

	// M·∫£ng l∆∞u tr·ªØ c√°c repo ƒë√£ th·ª≠ x·ª≠ l√Ω nh∆∞ng g·∫∑p l·ªói ƒë·ªÉ th·ª≠ l·∫°i sau
	var failedRepos []githubapi.GithubAPIResponse
	var failedReposMutex sync.Mutex

	// X·ª≠ l√Ω t·ª´ng repository trong danh s√°ch top
	for idx, repoSummary := range topRepos {
		select {
		case <-ctx.Done():
			c.Logger.Warn(ctx, "X·ª≠ l√Ω repositories b·ªã h·ªßy b·ªè do context ƒë√£ ƒë√≥ng")
			return
		case repoSemaphore <- struct{}{}: // √Åp d·ª•ng semaphore ƒë·ªÉ gi·ªõi h·∫°n s·ªë l∆∞·ª£ng goroutine ƒë·ªìng th·ªùi
			c.backgroundWg.Add(1)
			go func(repo githubapi.GithubAPIResponse, index int) {
				defer c.backgroundWg.Done()
				defer func() { <-repoSemaphore }()

				// T·∫°o transaction m·ªõi
				repoTx := db.Begin()
				if repoTx.Error != nil {
					c.errorChan <- repoTx.Error
					// Th√™m v√†o danh s√°ch c√°c repos c·∫ßn th·ª≠ l·∫°i
					failedReposMutex.Lock()
					failedRepos = append(failedRepos, repo)
					failedReposMutex.Unlock()
					return
				}

				defer func() {
					if r := recover(); r != nil {
						repoTx.Rollback()
						c.errorChan <- fmt.Errorf("panic x·∫£y ra trong goroutine x·ª≠ l√Ω repo: %v", r)
						// Th√™m v√†o danh s√°ch c√°c repos c·∫ßn th·ª≠ l·∫°i
						failedReposMutex.Lock()
						failedRepos = append(failedRepos, repo)
						failedReposMutex.Unlock()
					}
				}()

				// X·ª≠ l√Ω repository
				repoModel, isSkipped, err := c.crawlRepo(repoTx, repo)
				if err != nil {
					repoTx.Rollback()
					c.errorChan <- err
					// Th√™m v√†o danh s√°ch c√°c repos c·∫ßn th·ª≠ l·∫°i
					failedReposMutex.Lock()
					failedRepos = append(failedRepos, repo)
					failedReposMutex.Unlock()
					return
				}

				if isSkipped {
					repoTx.Rollback()
					return
				}

				// Commit transaction
				if err := repoTx.Commit().Error; err != nil {
					c.errorChan <- err
					// Th√™m v√†o danh s√°ch c√°c repos c·∫ßn th·ª≠ l·∫°i
					failedReposMutex.Lock()
					failedRepos = append(failedRepos, repo)
					failedReposMutex.Unlock()
					return
				}

				// TƒÉng counter
				newCount := atomic.AddInt32(&c.repoCount, 1)
				c.Logger.Info(ctx, "Ti·∫øn ƒë·ªô: %d/%d - ƒê√£ x·ª≠ l√Ω %s/%s (ID: %d, Stars: %d)",
					newCount, len(topRepos), repoModel.User, repoModel.Name, repoModel.ID, repoModel.StarCount)

				// X·ª≠ l√Ω releases v√† commits trong m·ªôt goroutine ri√™ng
				if index < 2000 { // Ch·ªâ x·ª≠ l√Ω releases v√† commits cho 2000 repo ƒë·∫ßu ti√™n v·ªõi s·ªë sao cao nh·∫•t
					c.backgroundWg.Add(1)
					go func(user, repoName string, repoID int) {
						defer c.backgroundWg.Done()
						releasesCtx := context.Background()
						c.crawlReleasesAndCommitsAsync(releasesCtx, db, user, repoName, repoID)
					}(repoModel.User, repoModel.Name, repoModel.ID)
				}
			}(repoSummary.APIRepo, idx)
		}
	}

	// ƒê·ª£i semaphore empty tr∆∞·ªõc khi ti·∫øp t·ª•c
	for i := 0; i < cap(repoSemaphore); i++ {
		repoSemaphore <- struct{}{}
	}

	// Th·ª≠ l·∫°i c√°c repos ƒë√£ th·∫•t b·∫°i (t·ªëi ƒëa 3 l·∫ßn)
	if len(failedRepos) > 0 {
		c.Logger.Info(ctx, "C√≥ %d repositories x·ª≠ l√Ω th·∫•t b·∫°i, ƒëang th·ª≠ l·∫°i", len(failedRepos))

		maxRetries := 3
		for retry := 0; retry < maxRetries; retry++ {
			if len(failedRepos) == 0 {
				break
			}

			c.Logger.Info(ctx, "L·∫ßn th·ª≠ l·∫°i th·ª© %d cho %d repositories", retry+1, len(failedRepos))

			// T·∫°o b·∫£n sao c·ªßa danh s√°ch repos th·∫•t b·∫°i v√† reset
			currentFailedRepos := failedRepos
			failedRepos = make([]githubapi.GithubAPIResponse, 0)

			// X·ª≠ l√Ω t·ª´ng repository trong danh s√°ch c√°c repos th·∫•t b·∫°i
			for _, repo := range currentFailedRepos {
				repoTx := db.Begin()
				if repoTx.Error != nil {
					continue
				}

				repoModel, isSkipped, err := c.crawlRepo(repoTx, repo)
				if err != nil || isSkipped {
					repoTx.Rollback()
					continue
				}

				// Commit transaction
				if err := repoTx.Commit().Error; err != nil {
					continue
				}

				// TƒÉng counter
				newCount := atomic.AddInt32(&c.repoCount, 1)
				c.Logger.Info(ctx, "Th·ª≠ l·∫°i th√†nh c√¥ng: %d/%d - ƒê√£ x·ª≠ l√Ω %s/%s (ID: %d, Stars: %d)",
					newCount, c.maxRepos, repoModel.User, repoModel.Name, repoModel.ID, repoModel.StarCount)
			}
		}
	}

	// Ki·ªÉm tra xem ƒë√£ x·ª≠ l√Ω ƒë·ªß s·ªë l∆∞·ª£ng repositories theo y√™u c·∫ßu ch∆∞a
	finalCount := atomic.LoadInt32(&c.repoCount)
	if finalCount < c.maxRepos {
		c.Logger.Warn(ctx, "Ch√∫ √Ω: Ch·ªâ x·ª≠ l√Ω ƒë∆∞·ª£c %d/%d repositories y√™u c·∫ßu", finalCount, c.maxRepos)

		// T√¨m th√™m repositories t·ª´ danh s√°ch ƒë√£ thu th·∫≠p n·∫øu c√≥ th·ªÉ
		if len(c.allRepos) > int(c.maxRepos) {
			remainingNeeded := int(c.maxRepos - finalCount)
			additionalRepos := c.allRepos[c.maxRepos:min(len(c.allRepos), int(c.maxRepos)+remainingNeeded)]

			c.Logger.Info(ctx, "X·ª≠ l√Ω th√™m %d repositories ƒë·ªÉ ƒë·∫°t target", len(additionalRepos))

			for _, repoSummary := range additionalRepos {
				repoTx := db.Begin()
				if repoTx.Error != nil {
					continue
				}

				repoModel, isSkipped, err := c.crawlRepo(repoTx, repoSummary.APIRepo)
				if err != nil || isSkipped {
					repoTx.Rollback()
					continue
				}

				if err := repoTx.Commit().Error; err != nil {
					continue
				}

				newCount := atomic.AddInt32(&c.repoCount, 1)
				c.Logger.Info(ctx, "B·ªï sung: %d/%d - ƒê√£ x·ª≠ l√Ω %s/%s (ID: %d, Stars: %d)",
					newCount, c.maxRepos, repoModel.User, repoModel.Name, repoModel.ID, repoModel.StarCount)

				if newCount >= c.maxRepos {
					break
				}
			}
		}
	}

	c.Logger.Info(ctx, "ƒê√£ ho√†n th√†nh x·ª≠ l√Ω repositories: %d/%d", atomic.LoadInt32(&c.repoCount), c.maxRepos)
}

func (c *CrawlerV3) logCrawlResults(ctx context.Context, startTime time.Time) {
	endTime := time.Now()
	duration := endTime.Sub(startTime)

	c.Logger.Info(ctx, "==== K·∫æT QU·∫¢ CRAWL V3 ====")
	c.Logger.Info(ctx, "Th·ªùi gian b·∫Øt ƒë·∫ßu: %s", startTime.Format(time.RFC3339))
	c.Logger.Info(ctx, "Th·ªùi gian k·∫øt th√∫c: %s", endTime.Format(time.RFC3339))
	c.Logger.Info(ctx, "T·ªïng th·ªùi gian th·ª±c hi·ªán: %v", duration)
	c.Logger.Info(ctx, "S·ªë l∆∞·ª£ng repositories ƒë√£ thu th·∫≠p th√¥ng tin: %d", len(c.allRepos))
	c.Logger.Info(ctx, "S·ªë l∆∞·ª£ng repositories ƒë√£ x·ª≠ l√Ω v√† l∆∞u v√†o database: %d", atomic.LoadInt32(&c.repoCount))
	c.Logger.Info(ctx, "S·ªë l∆∞·ª£ng releases ƒë√£ crawl: %d", atomic.LoadInt32(&c.releaseCount))
	c.Logger.Info(ctx, "S·ªë l∆∞·ª£ng commits ƒë√£ crawl: %d", atomic.LoadInt32(&c.commitCount))
}

// Phase 1: Thu th·∫≠p th√¥ng tin repositories
func (c *CrawlerV3) collectRepositoriesPhase(ctx context.Context, db *gorm.DB) {
	// T·∫°o k√™nh doneCh ƒë·ªÉ th√¥ng b√°o khi ƒë√£ thu th·∫≠p ƒë·ªß th√¥ng tin
	doneCh := make(chan bool)

	// Kh·ªüi ch·∫°y m·ªôt goroutine ki·ªÉm tra s·ªë l∆∞·ª£ng repositories ƒë√£ thu th·∫≠p ƒë∆∞·ª£c
	go func() {
		for {
			c.allReposMutex.Lock()
			repoCount := len(c.allRepos)
			c.allReposMutex.Unlock()

			if repoCount >= 10000 { // Thu th·∫≠p nhi·ªÅu h∆°n 5000 ƒë·ªÉ c√≥ th·ªÉ l·ªçc
				c.Logger.Info(ctx, "ƒê√£ thu th·∫≠p ƒë·ªß th√¥ng tin %d repositories, k·∫øt th√∫c giai ƒëo·∫°n 1", repoCount)
				close(doneCh)
				return
			}
			time.Sleep(2 * time.Second)
		}
	}()

	// Kh·ªüi ch·∫°y c√°c worker thu th·∫≠p repositories
	c.startTimeWindowWorkers(ctx, db, doneCh)

	// ƒê·ª£i c√°c worker ho√†n th√†nh ho·∫∑c h·∫øt th·ªùi gian
	waitCh := make(chan struct{})
	go func() {
		c.pageWaitGroup.Wait()
		close(waitCh)
	}()

	select {
	case <-waitCh:
		c.Logger.Info(ctx, "T·∫•t c·∫£ c√°c worker thu th·∫≠p repositories ƒë√£ ho√†n th√†nh")
	case <-time.After(30 * time.Minute): // Gi·ªõi h·∫°n th·ªùi gian cho giai ƒëo·∫°n 1
		c.Logger.Info(ctx, "H·∫øt th·ªùi gian cho giai ƒëo·∫°n thu th·∫≠p repositories")
	}

	// Hi·ªÉn th·ªã th√¥ng tin v·ªÅ repositories ƒë√£ thu th·∫≠p ƒë∆∞·ª£c
	c.allReposMutex.Lock()
	repoCount := len(c.allRepos)
	c.allReposMutex.Unlock()
	c.Logger.Info(ctx, "Giai ƒëo·∫°n 1 k·∫øt th√∫c: ƒê√£ thu th·∫≠p th√¥ng tin v·ªÅ %d repositories", repoCount)

	// Th√¥ng b√°o giai ƒëo·∫°n 1 ƒë√£ ho√†n th√†nh
	close(c.repoCollectionDone)
}

// Phase 2: X·ª≠ l√Ω repositories v√† thu th·∫≠p releases v√† commits
func (c *CrawlerV3) processRepositoriesPhase(ctx context.Context, db *gorm.DB) {
	// S·∫Øp x·∫øp v√† x·ª≠ l√Ω c√°c repositories h√†ng ƒë·∫ßu
	c.processTopRepositories(ctx, db)

	// ƒê·∫∑t m·ªôt th·ªùi gian t·ªëi ƒëa cho qu√° tr√¨nh x·ª≠ l√Ω
	processTimeout := 60 * time.Minute

	// T·∫°o m·ªôt k√™nh ƒë·ªÉ th√¥ng b√°o khi ƒë√£ ho√†n th√†nh
	waitCh := make(chan struct{})

	// Ch·ªù t·∫•t c·∫£ c√°c worker x·ª≠ l√Ω repositories ho√†n th√†nh
	go func() {
		c.backgroundWg.Wait()
		close(waitCh)
		// ƒê√°nh d·∫•u giai ƒëo·∫°n 2 ƒë√£ ho√†n th√†nh
		close(c.secondPhaseDone)
	}()

	// ƒê·ªãnh k·ª≥ b√°o c√°o ti·∫øn ƒë·ªô
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-waitCh:
			c.Logger.Info(ctx, "Giai ƒëo·∫°n 2 ƒë√£ ho√†n th√†nh: ƒê√£ x·ª≠ l√Ω t·∫•t c·∫£ repositories v√† d·ªØ li·ªáu li√™n quan")
			return
		case <-time.After(processTimeout):
			c.Logger.Warn(ctx, "H·∫øt th·ªùi gian cho giai ƒëo·∫°n 2 (%v), k·∫øt th√∫c qu√° tr√¨nh", processTimeout)
			return
		case <-ticker.C:
			// B√°o c√°o ti·∫øn ƒë·ªô hi·ªán t·∫°i
			c.Logger.Info(ctx, "Ti·∫øn ƒë·ªô giai ƒëo·∫°n 2: ƒê√£ x·ª≠ l√Ω %d repositories, %d releases, %d commits",
				atomic.LoadInt32(&c.repoCount),
				atomic.LoadInt32(&c.releaseCount),
				atomic.LoadInt32(&c.commitCount))
		}
	}
}

// helper function for integer minimum comparison
func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}
